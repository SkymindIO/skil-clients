/**
 * Endpoints
 * Endpoints API for different services in SKIL
 *
 * OpenAPI spec version: 1.2.0-beta
 * 
 *
 * NOTE: This class is auto generated by the swagger code generator program.
 * https://github.com/swagger-api/swagger-codegen.git
 * Do not edit the class manually.
 */

package ai.skymind.skil

import java.text.SimpleDateFormat

import ai.skymind.skil.model.Base64NDArrayBody
import ai.skymind.skil.model.ClassificationResult
import ai.skymind.skil.model.DetectionResult
import java.io.File
import ai.skymind.skil.model.JsonArrayResponse
import ai.skymind.skil.model.LogBatch
import ai.skymind.skil.model.LogRequest
import ai.skymind.skil.model.MetaData
import ai.skymind.skil.model.ModelStatus
import ai.skymind.skil.model.MultiClassClassificationResult
import ai.skymind.skil.model.MultiPredictRequest
import ai.skymind.skil.model.MultiPredictResponse
import ai.skymind.skil.model.Prediction
import io.swagger.client.{ApiInvoker, ApiException}

import com.sun.jersey.multipart.FormDataMultiPart
import com.sun.jersey.multipart.file.FileDataBodyPart

import javax.ws.rs.core.MediaType

import java.io.File
import java.util.Date
import java.util.TimeZone

import scala.collection.mutable.HashMap

import com.wordnik.swagger.client._
import scala.concurrent.Future
import collection.mutable

import java.net.URI

import com.wordnik.swagger.client.ClientResponseReaders.Json4sFormatsReader._
import com.wordnik.swagger.client.RequestWriters.Json4sFormatsWriter._

import scala.concurrent.ExecutionContext.Implicits.global
import scala.concurrent._
import scala.concurrent.duration._
import scala.util.{Failure, Success, Try}

import org.json4s._

class InferenceApi(
  val defBasePath: String = "http://localhost:9008",
  defApiInvoker: ApiInvoker = ApiInvoker
) {
  private lazy val dateTimeFormatter = {
    val formatter = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss.SSSZ")
    formatter.setTimeZone(TimeZone.getTimeZone("UTC"))
    formatter
  }
  private val dateFormatter = {
    val formatter = new SimpleDateFormat("yyyy-MM-dd")
    formatter.setTimeZone(TimeZone.getTimeZone("UTC"))
    formatter
  }
  implicit val formats = new org.json4s.DefaultFormats {
    override def dateFormatter = dateTimeFormatter
  }
  implicit val stringReader: ClientResponseReader[String] = ClientResponseReaders.StringReader
  implicit val unitReader: ClientResponseReader[Unit] = ClientResponseReaders.UnitReader
  implicit val jvalueReader: ClientResponseReader[JValue] = ClientResponseReaders.JValueReader
  implicit val jsonReader: ClientResponseReader[Nothing] = JsonFormatsReader
  implicit val stringWriter: RequestWriter[String] = RequestWriters.StringWriter
  implicit val jsonWriter: RequestWriter[Nothing] = JsonFormatsWriter

  var basePath: String = defBasePath
  var apiInvoker: ApiInvoker = defApiInvoker

  def addHeader(key: String, value: String): mutable.HashMap[String, String] = {
    apiInvoker.defaultHeaders += key -> value
  }

  val config: SwaggerConfig = SwaggerConfig.forUrl(new URI(defBasePath))
  val client = new RestClient(config)
  val helper = new InferenceApiAsyncHelper(client, config)

  /**
   * Use the deployed model to classify the input
   * 
   *
   * @param Body The input NDArray 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return ClassificationResult
   */
  def classify(Body: Prediction, DeploymentName: String, VersionName: String, ModelName: String): Option[ClassificationResult] = {
    val await = Try(Await.result(classifyAsync(Body, DeploymentName, VersionName, ModelName), Duration.Inf))
    await match {
      case Success(i) => Some(await.get)
      case Failure(t) => None
    }
  }

  /**
   * Use the deployed model to classify the input asynchronously
   * 
   *
   * @param Body The input NDArray 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return Future(ClassificationResult)
   */
  def classifyAsync(Body: Prediction, DeploymentName: String, VersionName: String, ModelName: String): Future[ClassificationResult] = {
      helper.classify(Body, DeploymentName, VersionName, ModelName)
  }

  /**
   * Same as /classify but returns the output as Base64NDArrayBody
   * 
   *
   * @param Body The input NDArray 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return Base64NDArrayBody
   */
  def classifyarray(Body: Prediction, DeploymentName: String, VersionName: String, ModelName: String): Option[Base64NDArrayBody] = {
    val await = Try(Await.result(classifyarrayAsync(Body, DeploymentName, VersionName, ModelName), Duration.Inf))
    await match {
      case Success(i) => Some(await.get)
      case Failure(t) => None
    }
  }

  /**
   * Same as /classify but returns the output as Base64NDArrayBody asynchronously
   * 
   *
   * @param Body The input NDArray 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return Future(Base64NDArrayBody)
   */
  def classifyarrayAsync(Body: Prediction, DeploymentName: String, VersionName: String, ModelName: String): Future[Base64NDArrayBody] = {
      helper.classifyarray(Body, DeploymentName, VersionName, ModelName)
  }

  /**
   * Use the deployed model to classify the input, using input image file from multipart form data.
   * 
   *
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @param Image The file to upload. (optional)
   * @return ClassificationResult
   */
  def classifyimage(DeploymentName: String, VersionName: String, ModelName: String, Image: Option[File] = None): Option[ClassificationResult] = {
    val await = Try(Await.result(classifyimageAsync(DeploymentName, VersionName, ModelName, Image), Duration.Inf))
    await match {
      case Success(i) => Some(await.get)
      case Failure(t) => None
    }
  }

  /**
   * Use the deployed model to classify the input, using input image file from multipart form data. asynchronously
   * 
   *
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @param Image The file to upload. (optional)
   * @return Future(ClassificationResult)
   */
  def classifyimageAsync(DeploymentName: String, VersionName: String, ModelName: String, Image: Option[File] = None): Future[ClassificationResult] = {
      helper.classifyimage(DeploymentName, VersionName, ModelName, Image)
  }

  /**
   * Detect the objects, given a (input) prediction request
   * 
   *
   * @param Id the GUID for mapping the results in the detections 
   * @param NeedsPreprocessing (true) if the image needs preprocessing 
   * @param Threshold A threshold, indicating the required surety for detecting a bounding box. For example, a threshold of 0.1 might give thousand bounding boxes for an image and a threshold of 0.99 might give none. 
   * @param File the image file to detect objects from 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return DetectionResult
   */
  def detectobjects(Id: String, NeedsPreprocessing: Boolean, Threshold: Float, File: File, DeploymentName: String, VersionName: String, ModelName: String): Option[DetectionResult] = {
    val await = Try(Await.result(detectobjectsAsync(Id, NeedsPreprocessing, Threshold, File, DeploymentName, VersionName, ModelName), Duration.Inf))
    await match {
      case Success(i) => Some(await.get)
      case Failure(t) => None
    }
  }

  /**
   * Detect the objects, given a (input) prediction request asynchronously
   * 
   *
   * @param Id the GUID for mapping the results in the detections 
   * @param NeedsPreprocessing (true) if the image needs preprocessing 
   * @param Threshold A threshold, indicating the required surety for detecting a bounding box. For example, a threshold of 0.1 might give thousand bounding boxes for an image and a threshold of 0.99 might give none. 
   * @param File the image file to detect objects from 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return Future(DetectionResult)
   */
  def detectobjectsAsync(Id: String, NeedsPreprocessing: Boolean, Threshold: Float, File: File, DeploymentName: String, VersionName: String, ModelName: String): Future[DetectionResult] = {
      helper.detectobjects(Id, NeedsPreprocessing, Threshold, File, DeploymentName, VersionName, ModelName)
  }

  /**
   * Run inference on the input and returns it as a JsonArrayResponse
   * 
   *
   * @param Body The input NDArray 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return JsonArrayResponse
   */
  def jsonarray(Body: Prediction, DeploymentName: String, VersionName: String, ModelName: String): Option[JsonArrayResponse] = {
    val await = Try(Await.result(jsonarrayAsync(Body, DeploymentName, VersionName, ModelName), Duration.Inf))
    await match {
      case Success(i) => Some(await.get)
      case Failure(t) => None
    }
  }

  /**
   * Run inference on the input and returns it as a JsonArrayResponse asynchronously
   * 
   *
   * @param Body The input NDArray 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return Future(JsonArrayResponse)
   */
  def jsonarrayAsync(Body: Prediction, DeploymentName: String, VersionName: String, ModelName: String): Future[JsonArrayResponse] = {
      helper.jsonarray(Body, DeploymentName, VersionName, ModelName)
  }

  /**
   * Get logs file path
   * 
   *
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return String
   */
  def logfilepath(DeploymentName: String, VersionName: String, ModelName: String): Option[String] = {
    val await = Try(Await.result(logfilepathAsync(DeploymentName, VersionName, ModelName), Duration.Inf))
    await match {
      case Success(i) => Some(await.get)
      case Failure(t) => None
    }
  }

  /**
   * Get logs file path asynchronously
   * 
   *
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return Future(String)
   */
  def logfilepathAsync(DeploymentName: String, VersionName: String, ModelName: String): Future[String] = {
      helper.logfilepath(DeploymentName, VersionName, ModelName)
  }

  /**
   * Get logs
   * 
   *
   * @param Body the the log request 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return LogBatch
   */
  def logs(Body: LogRequest, DeploymentName: String, VersionName: String, ModelName: String): Option[LogBatch] = {
    val await = Try(Await.result(logsAsync(Body, DeploymentName, VersionName, ModelName), Duration.Inf))
    await match {
      case Success(i) => Some(await.get)
      case Failure(t) => None
    }
  }

  /**
   * Get logs asynchronously
   * 
   *
   * @param Body the the log request 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return Future(LogBatch)
   */
  def logsAsync(Body: LogRequest, DeploymentName: String, VersionName: String, ModelName: String): Future[LogBatch] = {
      helper.logs(Body, DeploymentName, VersionName, ModelName)
  }

  /**
   * this method can be used to get the meta data for the current model which set to the server
   * 
   *
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return MetaData
   */
  def metaGet(DeploymentName: String, VersionName: String, ModelName: String): Option[MetaData] = {
    val await = Try(Await.result(metaGetAsync(DeploymentName, VersionName, ModelName), Duration.Inf))
    await match {
      case Success(i) => Some(await.get)
      case Failure(t) => None
    }
  }

  /**
   * this method can be used to get the meta data for the current model which set to the server asynchronously
   * 
   *
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return Future(MetaData)
   */
  def metaGetAsync(DeploymentName: String, VersionName: String, ModelName: String): Future[MetaData] = {
      helper.metaGet(DeploymentName, VersionName, ModelName)
  }

  /**
   * This method can be used to set meta data for the current model which is set to the server
   * 
   *
   * @param Body the meta data object 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return MetaData
   */
  def metaPost(Body: MetaData, DeploymentName: String, VersionName: String, ModelName: String): Option[MetaData] = {
    val await = Try(Await.result(metaPostAsync(Body, DeploymentName, VersionName, ModelName), Duration.Inf))
    await match {
      case Success(i) => Some(await.get)
      case Failure(t) => None
    }
  }

  /**
   * This method can be used to set meta data for the current model which is set to the server asynchronously
   * 
   *
   * @param Body the meta data object 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return Future(MetaData)
   */
  def metaPostAsync(Body: MetaData, DeploymentName: String, VersionName: String, ModelName: String): Future[MetaData] = {
      helper.metaPost(Body, DeploymentName, VersionName, ModelName)
  }

  /**
   * Set the model to be served
   * 
   *
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @param File The model file to upload (.pb file) (optional)
   * @return ModelStatus
   */
  def modelset(DeploymentName: String, VersionName: String, ModelName: String, File: Option[File] = None): Option[ModelStatus] = {
    val await = Try(Await.result(modelsetAsync(DeploymentName, VersionName, ModelName, File), Duration.Inf))
    await match {
      case Success(i) => Some(await.get)
      case Failure(t) => None
    }
  }

  /**
   * Set the model to be served asynchronously
   * 
   *
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @param File The model file to upload (.pb file) (optional)
   * @return Future(ModelStatus)
   */
  def modelsetAsync(DeploymentName: String, VersionName: String, ModelName: String, File: Option[File] = None): Future[ModelStatus] = {
      helper.modelset(DeploymentName, VersionName, ModelName, File)
  }

  /**
   * Update the model to be served
   * 
   *
   * @param File The model file to update with (.pb file) 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return ModelStatus
   */
  def modelupdate(File: File, DeploymentName: String, VersionName: String, ModelName: String): Option[ModelStatus] = {
    val await = Try(Await.result(modelupdateAsync(File, DeploymentName, VersionName, ModelName), Duration.Inf))
    await match {
      case Success(i) => Some(await.get)
      case Failure(t) => None
    }
  }

  /**
   * Update the model to be served asynchronously
   * 
   *
   * @param File The model file to update with (.pb file) 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return Future(ModelStatus)
   */
  def modelupdateAsync(File: File, DeploymentName: String, VersionName: String, ModelName: String): Future[ModelStatus] = {
      helper.modelupdate(File, DeploymentName, VersionName, ModelName)
  }

  /**
   * Represents all of the labels for a given classification
   * 
   *
   * @param Body The input NDArray 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return MultiClassClassificationResult
   */
  def multiclassify(Body: Prediction, DeploymentName: String, VersionName: String, ModelName: String): Option[MultiClassClassificationResult] = {
    val await = Try(Await.result(multiclassifyAsync(Body, DeploymentName, VersionName, ModelName), Duration.Inf))
    await match {
      case Success(i) => Some(await.get)
      case Failure(t) => None
    }
  }

  /**
   * Represents all of the labels for a given classification asynchronously
   * 
   *
   * @param Body The input NDArray 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return Future(MultiClassClassificationResult)
   */
  def multiclassifyAsync(Body: Prediction, DeploymentName: String, VersionName: String, ModelName: String): Future[MultiClassClassificationResult] = {
      helper.multiclassify(Body, DeploymentName, VersionName, ModelName)
  }

  /**
   * Get the output from the network, based on the given INDArray[] input
   * Networks with multiple input/output are supported via this method. A Normalizer will be used if needsPreProcessing is set to true. The output/returned array of INDArray will be the raw predictions, and consequently this method can be used for classification or regression networks, with any type of output layer (standard, time series / RnnOutputLayer, etc).
   *
   * @param Body The multiple input arrays with mask inputs to run inferences on 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return MultiPredictResponse
   */
  def multipredict(Body: MultiPredictRequest, DeploymentName: String, VersionName: String, ModelName: String): Option[MultiPredictResponse] = {
    val await = Try(Await.result(multipredictAsync(Body, DeploymentName, VersionName, ModelName), Duration.Inf))
    await match {
      case Success(i) => Some(await.get)
      case Failure(t) => None
    }
  }

  /**
   * Get the output from the network, based on the given INDArray[] input asynchronously
   * Networks with multiple input/output are supported via this method. A Normalizer will be used if needsPreProcessing is set to true. The output/returned array of INDArray will be the raw predictions, and consequently this method can be used for classification or regression networks, with any type of output layer (standard, time series / RnnOutputLayer, etc).
   *
   * @param Body The multiple input arrays with mask inputs to run inferences on 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return Future(MultiPredictResponse)
   */
  def multipredictAsync(Body: MultiPredictRequest, DeploymentName: String, VersionName: String, ModelName: String): Future[MultiPredictResponse] = {
      helper.multipredict(Body, DeploymentName, VersionName, ModelName)
  }

  /**
   * Get the output from the network using the given image file using the /multipredict endpoint&#39;s method
   * Networks with multiple input/output are supported via this method. A Normalizer will be used if needsPreProcessing is set to true. The output/returned array of INDArray will be the raw predictions, and consequently this method can be used for classification or regression networks, with any type of output layer (standard, time series / RnnOutputLayer, etc).
   *
   * @param File The image file to run the prediction on 
   * @param Id The id of the request (could be self generated) 
   * @param NeedsPreprocessing Whether or not the preprocessing is required (either &#39;true&#39; or &#39;false&#39;) 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return MultiPredictResponse
   */
  def multipredictimage(File: File, Id: String, NeedsPreprocessing: Boolean, DeploymentName: String, VersionName: String, ModelName: String): Option[MultiPredictResponse] = {
    val await = Try(Await.result(multipredictimageAsync(File, Id, NeedsPreprocessing, DeploymentName, VersionName, ModelName), Duration.Inf))
    await match {
      case Success(i) => Some(await.get)
      case Failure(t) => None
    }
  }

  /**
   * Get the output from the network using the given image file using the /multipredict endpoint&#39;s method asynchronously
   * Networks with multiple input/output are supported via this method. A Normalizer will be used if needsPreProcessing is set to true. The output/returned array of INDArray will be the raw predictions, and consequently this method can be used for classification or regression networks, with any type of output layer (standard, time series / RnnOutputLayer, etc).
   *
   * @param File The image file to run the prediction on 
   * @param Id The id of the request (could be self generated) 
   * @param NeedsPreprocessing Whether or not the preprocessing is required (either &#39;true&#39; or &#39;false&#39;) 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return Future(MultiPredictResponse)
   */
  def multipredictimageAsync(File: File, Id: String, NeedsPreprocessing: Boolean, DeploymentName: String, VersionName: String, ModelName: String): Future[MultiPredictResponse] = {
      helper.multipredictimage(File, Id, NeedsPreprocessing, DeploymentName, VersionName, ModelName)
  }

  /**
   * Run inference on the input array.
   * 
   *
   * @param Body The input NDArray 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return Prediction
   */
  def predict(Body: Prediction, DeploymentName: String, VersionName: String, ModelName: String): Option[Prediction] = {
    val await = Try(Await.result(predictAsync(Body, DeploymentName, VersionName, ModelName), Duration.Inf))
    await match {
      case Success(i) => Some(await.get)
      case Failure(t) => None
    }
  }

  /**
   * Run inference on the input array. asynchronously
   * 
   *
   * @param Body The input NDArray 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return Future(Prediction)
   */
  def predictAsync(Body: Prediction, DeploymentName: String, VersionName: String, ModelName: String): Future[Prediction] = {
      helper.predict(Body, DeploymentName, VersionName, ModelName)
  }

  /**
   * Run inference on the input array, using input image file from multipart form data.
   * 
   *
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @param Image The file to upload. (optional)
   * @return Prediction
   */
  def predictimage(DeploymentName: String, VersionName: String, ModelName: String, Image: Option[File] = None): Option[Prediction] = {
    val await = Try(Await.result(predictimageAsync(DeploymentName, VersionName, ModelName, Image), Duration.Inf))
    await match {
      case Success(i) => Some(await.get)
      case Failure(t) => None
    }
  }

  /**
   * Run inference on the input array, using input image file from multipart form data. asynchronously
   * 
   *
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @param Image The file to upload. (optional)
   * @return Future(Prediction)
   */
  def predictimageAsync(DeploymentName: String, VersionName: String, ModelName: String, Image: Option[File] = None): Future[Prediction] = {
      helper.predictimage(DeploymentName, VersionName, ModelName, Image)
  }

  /**
   * Preprocesses the input and run inference on it
   * 
   *
   * @param Body The input array 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return Prediction
   */
  def predictwithpreprocess(Body: List[String], DeploymentName: String, VersionName: String, ModelName: String): Option[Prediction] = {
    val await = Try(Await.result(predictwithpreprocessAsync(Body, DeploymentName, VersionName, ModelName), Duration.Inf))
    await match {
      case Success(i) => Some(await.get)
      case Failure(t) => None
    }
  }

  /**
   * Preprocesses the input and run inference on it asynchronously
   * 
   *
   * @param Body The input array 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return Future(Prediction)
   */
  def predictwithpreprocessAsync(Body: List[String], DeploymentName: String, VersionName: String, ModelName: String): Future[Prediction] = {
      helper.predictwithpreprocess(Body, DeploymentName, VersionName, ModelName)
  }

  /**
   * Preprocesses the input and run inference on it and returns it as a JsonArrayResponse
   * 
   *
   * @param Body The input array 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return JsonArrayResponse
   */
  def predictwithpreprocessjson(Body: List[String], DeploymentName: String, VersionName: String, ModelName: String): Option[JsonArrayResponse] = {
    val await = Try(Await.result(predictwithpreprocessjsonAsync(Body, DeploymentName, VersionName, ModelName), Duration.Inf))
    await match {
      case Success(i) => Some(await.get)
      case Failure(t) => None
    }
  }

  /**
   * Preprocesses the input and run inference on it and returns it as a JsonArrayResponse asynchronously
   * 
   *
   * @param Body The input array 
   * @param DeploymentName Name of the deployment group 
   * @param VersionName Version name of the endpoint. The default value is \&quot;default\&quot; 
   * @param ModelName ID or name of the deployed model 
   * @return Future(JsonArrayResponse)
   */
  def predictwithpreprocessjsonAsync(Body: List[String], DeploymentName: String, VersionName: String, ModelName: String): Future[JsonArrayResponse] = {
      helper.predictwithpreprocessjson(Body, DeploymentName, VersionName, ModelName)
  }

}

class InferenceApiAsyncHelper(client: TransportClient, config: SwaggerConfig) extends ApiClient(client, config) {

  def classify(Body: Prediction,
    DeploymentName: String,
    VersionName: String,
    ModelName: String)(implicit reader: ClientResponseReader[ClassificationResult], writer: RequestWriter[Prediction]): Future[ClassificationResult] = {
    // create path and map variables
    val path = (addFmt("/endpoints/{deploymentName}/model/{modelName}/{versionName}/classify")
      replaceAll("\\{" + "deploymentName" + "\\}", DeploymentName.toString)
      replaceAll("\\{" + "versionName" + "\\}", VersionName.toString)
      replaceAll("\\{" + "modelName" + "\\}", ModelName.toString))

    // query params
    val queryParams = new mutable.HashMap[String, String]
    val headerParams = new mutable.HashMap[String, String]

    if (Body == null) throw new Exception("Missing required parameter 'Body' when calling InferenceApi->classify")
    if (DeploymentName == null) throw new Exception("Missing required parameter 'DeploymentName' when calling InferenceApi->classify")

    if (VersionName == null) throw new Exception("Missing required parameter 'VersionName' when calling InferenceApi->classify")

    if (ModelName == null) throw new Exception("Missing required parameter 'ModelName' when calling InferenceApi->classify")


    val resFuture = client.submit("POST", path, queryParams.toMap, headerParams.toMap, writer.write(Body))
    resFuture flatMap { resp =>
      process(reader.read(resp))
    }
  }

  def classifyarray(Body: Prediction,
    DeploymentName: String,
    VersionName: String,
    ModelName: String)(implicit reader: ClientResponseReader[Base64NDArrayBody], writer: RequestWriter[Prediction]): Future[Base64NDArrayBody] = {
    // create path and map variables
    val path = (addFmt("/endpoints/{deploymentName}/model/{modelName}/{versionName}/classifyarray")
      replaceAll("\\{" + "deploymentName" + "\\}", DeploymentName.toString)
      replaceAll("\\{" + "versionName" + "\\}", VersionName.toString)
      replaceAll("\\{" + "modelName" + "\\}", ModelName.toString))

    // query params
    val queryParams = new mutable.HashMap[String, String]
    val headerParams = new mutable.HashMap[String, String]

    if (Body == null) throw new Exception("Missing required parameter 'Body' when calling InferenceApi->classifyarray")
    if (DeploymentName == null) throw new Exception("Missing required parameter 'DeploymentName' when calling InferenceApi->classifyarray")

    if (VersionName == null) throw new Exception("Missing required parameter 'VersionName' when calling InferenceApi->classifyarray")

    if (ModelName == null) throw new Exception("Missing required parameter 'ModelName' when calling InferenceApi->classifyarray")


    val resFuture = client.submit("POST", path, queryParams.toMap, headerParams.toMap, writer.write(Body))
    resFuture flatMap { resp =>
      process(reader.read(resp))
    }
  }

  def classifyimage(DeploymentName: String,
    VersionName: String,
    ModelName: String,
    Image: Option[File] = None
    )(implicit reader: ClientResponseReader[ClassificationResult]): Future[ClassificationResult] = {
    // create path and map variables
    val path = (addFmt("/endpoints/{deploymentName}/model/{modelName}/{versionName}/classifyimage")
      replaceAll("\\{" + "deploymentName" + "\\}", DeploymentName.toString)
      replaceAll("\\{" + "versionName" + "\\}", VersionName.toString)
      replaceAll("\\{" + "modelName" + "\\}", ModelName.toString))

    // query params
    val queryParams = new mutable.HashMap[String, String]
    val headerParams = new mutable.HashMap[String, String]

    if (DeploymentName == null) throw new Exception("Missing required parameter 'DeploymentName' when calling InferenceApi->classifyimage")

    if (VersionName == null) throw new Exception("Missing required parameter 'VersionName' when calling InferenceApi->classifyimage")

    if (ModelName == null) throw new Exception("Missing required parameter 'ModelName' when calling InferenceApi->classifyimage")


    val resFuture = client.submit("POST", path, queryParams.toMap, headerParams.toMap, "")
    resFuture flatMap { resp =>
      process(reader.read(resp))
    }
  }

  def detectobjects(Id: String,
    NeedsPreprocessing: Boolean,
    Threshold: Float,
    File: File,
    DeploymentName: String,
    VersionName: String,
    ModelName: String)(implicit reader: ClientResponseReader[DetectionResult]): Future[DetectionResult] = {
    // create path and map variables
    val path = (addFmt("/endpoints/{deploymentName}/model/{modelName}/{versionName}/detectobjects")
      replaceAll("\\{" + "deploymentName" + "\\}", DeploymentName.toString)
      replaceAll("\\{" + "versionName" + "\\}", VersionName.toString)
      replaceAll("\\{" + "modelName" + "\\}", ModelName.toString))

    // query params
    val queryParams = new mutable.HashMap[String, String]
    val headerParams = new mutable.HashMap[String, String]

    if (Id == null) throw new Exception("Missing required parameter 'Id' when calling InferenceApi->detectobjects")

    if (File == null) throw new Exception("Missing required parameter 'File' when calling InferenceApi->detectobjects")
    if (DeploymentName == null) throw new Exception("Missing required parameter 'DeploymentName' when calling InferenceApi->detectobjects")

    if (VersionName == null) throw new Exception("Missing required parameter 'VersionName' when calling InferenceApi->detectobjects")

    if (ModelName == null) throw new Exception("Missing required parameter 'ModelName' when calling InferenceApi->detectobjects")


    val resFuture = client.submit("POST", path, queryParams.toMap, headerParams.toMap, "")
    resFuture flatMap { resp =>
      process(reader.read(resp))
    }
  }

  def jsonarray(Body: Prediction,
    DeploymentName: String,
    VersionName: String,
    ModelName: String)(implicit reader: ClientResponseReader[JsonArrayResponse], writer: RequestWriter[Prediction]): Future[JsonArrayResponse] = {
    // create path and map variables
    val path = (addFmt("/endpoints/{deploymentName}/model/{modelName}/{versionName}/jsonarray")
      replaceAll("\\{" + "deploymentName" + "\\}", DeploymentName.toString)
      replaceAll("\\{" + "versionName" + "\\}", VersionName.toString)
      replaceAll("\\{" + "modelName" + "\\}", ModelName.toString))

    // query params
    val queryParams = new mutable.HashMap[String, String]
    val headerParams = new mutable.HashMap[String, String]

    if (Body == null) throw new Exception("Missing required parameter 'Body' when calling InferenceApi->jsonarray")
    if (DeploymentName == null) throw new Exception("Missing required parameter 'DeploymentName' when calling InferenceApi->jsonarray")

    if (VersionName == null) throw new Exception("Missing required parameter 'VersionName' when calling InferenceApi->jsonarray")

    if (ModelName == null) throw new Exception("Missing required parameter 'ModelName' when calling InferenceApi->jsonarray")


    val resFuture = client.submit("POST", path, queryParams.toMap, headerParams.toMap, writer.write(Body))
    resFuture flatMap { resp =>
      process(reader.read(resp))
    }
  }

  def logfilepath(DeploymentName: String,
    VersionName: String,
    ModelName: String)(implicit reader: ClientResponseReader[String]): Future[String] = {
    // create path and map variables
    val path = (addFmt("/endpoints/{deploymentName}/model/{modelName}/{versionName}/logfilepath")
      replaceAll("\\{" + "deploymentName" + "\\}", DeploymentName.toString)
      replaceAll("\\{" + "versionName" + "\\}", VersionName.toString)
      replaceAll("\\{" + "modelName" + "\\}", ModelName.toString))

    // query params
    val queryParams = new mutable.HashMap[String, String]
    val headerParams = new mutable.HashMap[String, String]

    if (DeploymentName == null) throw new Exception("Missing required parameter 'DeploymentName' when calling InferenceApi->logfilepath")

    if (VersionName == null) throw new Exception("Missing required parameter 'VersionName' when calling InferenceApi->logfilepath")

    if (ModelName == null) throw new Exception("Missing required parameter 'ModelName' when calling InferenceApi->logfilepath")


    val resFuture = client.submit("GET", path, queryParams.toMap, headerParams.toMap, "")
    resFuture flatMap { resp =>
      process(reader.read(resp))
    }
  }

  def logs(Body: LogRequest,
    DeploymentName: String,
    VersionName: String,
    ModelName: String)(implicit reader: ClientResponseReader[LogBatch], writer: RequestWriter[LogRequest]): Future[LogBatch] = {
    // create path and map variables
    val path = (addFmt("/endpoints/{deploymentName}/model/{modelName}/{versionName}/logs")
      replaceAll("\\{" + "deploymentName" + "\\}", DeploymentName.toString)
      replaceAll("\\{" + "versionName" + "\\}", VersionName.toString)
      replaceAll("\\{" + "modelName" + "\\}", ModelName.toString))

    // query params
    val queryParams = new mutable.HashMap[String, String]
    val headerParams = new mutable.HashMap[String, String]

    if (Body == null) throw new Exception("Missing required parameter 'Body' when calling InferenceApi->logs")
    if (DeploymentName == null) throw new Exception("Missing required parameter 'DeploymentName' when calling InferenceApi->logs")

    if (VersionName == null) throw new Exception("Missing required parameter 'VersionName' when calling InferenceApi->logs")

    if (ModelName == null) throw new Exception("Missing required parameter 'ModelName' when calling InferenceApi->logs")


    val resFuture = client.submit("POST", path, queryParams.toMap, headerParams.toMap, writer.write(Body))
    resFuture flatMap { resp =>
      process(reader.read(resp))
    }
  }

  def metaGet(DeploymentName: String,
    VersionName: String,
    ModelName: String)(implicit reader: ClientResponseReader[MetaData]): Future[MetaData] = {
    // create path and map variables
    val path = (addFmt("/endpoints/{deploymentName}/model/{modelName}/{versionName}/meta")
      replaceAll("\\{" + "deploymentName" + "\\}", DeploymentName.toString)
      replaceAll("\\{" + "versionName" + "\\}", VersionName.toString)
      replaceAll("\\{" + "modelName" + "\\}", ModelName.toString))

    // query params
    val queryParams = new mutable.HashMap[String, String]
    val headerParams = new mutable.HashMap[String, String]

    if (DeploymentName == null) throw new Exception("Missing required parameter 'DeploymentName' when calling InferenceApi->metaGet")

    if (VersionName == null) throw new Exception("Missing required parameter 'VersionName' when calling InferenceApi->metaGet")

    if (ModelName == null) throw new Exception("Missing required parameter 'ModelName' when calling InferenceApi->metaGet")


    val resFuture = client.submit("GET", path, queryParams.toMap, headerParams.toMap, "")
    resFuture flatMap { resp =>
      process(reader.read(resp))
    }
  }

  def metaPost(Body: MetaData,
    DeploymentName: String,
    VersionName: String,
    ModelName: String)(implicit reader: ClientResponseReader[MetaData], writer: RequestWriter[MetaData]): Future[MetaData] = {
    // create path and map variables
    val path = (addFmt("/endpoints/{deploymentName}/model/{modelName}/{versionName}/meta")
      replaceAll("\\{" + "deploymentName" + "\\}", DeploymentName.toString)
      replaceAll("\\{" + "versionName" + "\\}", VersionName.toString)
      replaceAll("\\{" + "modelName" + "\\}", ModelName.toString))

    // query params
    val queryParams = new mutable.HashMap[String, String]
    val headerParams = new mutable.HashMap[String, String]

    if (Body == null) throw new Exception("Missing required parameter 'Body' when calling InferenceApi->metaPost")
    if (DeploymentName == null) throw new Exception("Missing required parameter 'DeploymentName' when calling InferenceApi->metaPost")

    if (VersionName == null) throw new Exception("Missing required parameter 'VersionName' when calling InferenceApi->metaPost")

    if (ModelName == null) throw new Exception("Missing required parameter 'ModelName' when calling InferenceApi->metaPost")


    val resFuture = client.submit("POST", path, queryParams.toMap, headerParams.toMap, writer.write(Body))
    resFuture flatMap { resp =>
      process(reader.read(resp))
    }
  }

  def modelset(DeploymentName: String,
    VersionName: String,
    ModelName: String,
    File: Option[File] = None
    )(implicit reader: ClientResponseReader[ModelStatus]): Future[ModelStatus] = {
    // create path and map variables
    val path = (addFmt("/endpoints/{deploymentName}/model/{modelName}/{versionName}/modelset")
      replaceAll("\\{" + "deploymentName" + "\\}", DeploymentName.toString)
      replaceAll("\\{" + "versionName" + "\\}", VersionName.toString)
      replaceAll("\\{" + "modelName" + "\\}", ModelName.toString))

    // query params
    val queryParams = new mutable.HashMap[String, String]
    val headerParams = new mutable.HashMap[String, String]

    if (DeploymentName == null) throw new Exception("Missing required parameter 'DeploymentName' when calling InferenceApi->modelset")

    if (VersionName == null) throw new Exception("Missing required parameter 'VersionName' when calling InferenceApi->modelset")

    if (ModelName == null) throw new Exception("Missing required parameter 'ModelName' when calling InferenceApi->modelset")


    val resFuture = client.submit("POST", path, queryParams.toMap, headerParams.toMap, "")
    resFuture flatMap { resp =>
      process(reader.read(resp))
    }
  }

  def modelupdate(File: File,
    DeploymentName: String,
    VersionName: String,
    ModelName: String)(implicit reader: ClientResponseReader[ModelStatus]): Future[ModelStatus] = {
    // create path and map variables
    val path = (addFmt("/endpoints/{deploymentName}/model/{modelName}/{versionName}/modelupdate")
      replaceAll("\\{" + "deploymentName" + "\\}", DeploymentName.toString)
      replaceAll("\\{" + "versionName" + "\\}", VersionName.toString)
      replaceAll("\\{" + "modelName" + "\\}", ModelName.toString))

    // query params
    val queryParams = new mutable.HashMap[String, String]
    val headerParams = new mutable.HashMap[String, String]

    if (File == null) throw new Exception("Missing required parameter 'File' when calling InferenceApi->modelupdate")
    if (DeploymentName == null) throw new Exception("Missing required parameter 'DeploymentName' when calling InferenceApi->modelupdate")

    if (VersionName == null) throw new Exception("Missing required parameter 'VersionName' when calling InferenceApi->modelupdate")

    if (ModelName == null) throw new Exception("Missing required parameter 'ModelName' when calling InferenceApi->modelupdate")


    val resFuture = client.submit("POST", path, queryParams.toMap, headerParams.toMap, "")
    resFuture flatMap { resp =>
      process(reader.read(resp))
    }
  }

  def multiclassify(Body: Prediction,
    DeploymentName: String,
    VersionName: String,
    ModelName: String)(implicit reader: ClientResponseReader[MultiClassClassificationResult], writer: RequestWriter[Prediction]): Future[MultiClassClassificationResult] = {
    // create path and map variables
    val path = (addFmt("/endpoints/{deploymentName}/model/{modelName}/{versionName}/multiclassify")
      replaceAll("\\{" + "deploymentName" + "\\}", DeploymentName.toString)
      replaceAll("\\{" + "versionName" + "\\}", VersionName.toString)
      replaceAll("\\{" + "modelName" + "\\}", ModelName.toString))

    // query params
    val queryParams = new mutable.HashMap[String, String]
    val headerParams = new mutable.HashMap[String, String]

    if (Body == null) throw new Exception("Missing required parameter 'Body' when calling InferenceApi->multiclassify")
    if (DeploymentName == null) throw new Exception("Missing required parameter 'DeploymentName' when calling InferenceApi->multiclassify")

    if (VersionName == null) throw new Exception("Missing required parameter 'VersionName' when calling InferenceApi->multiclassify")

    if (ModelName == null) throw new Exception("Missing required parameter 'ModelName' when calling InferenceApi->multiclassify")


    val resFuture = client.submit("POST", path, queryParams.toMap, headerParams.toMap, writer.write(Body))
    resFuture flatMap { resp =>
      process(reader.read(resp))
    }
  }

  def multipredict(Body: MultiPredictRequest,
    DeploymentName: String,
    VersionName: String,
    ModelName: String)(implicit reader: ClientResponseReader[MultiPredictResponse], writer: RequestWriter[MultiPredictRequest]): Future[MultiPredictResponse] = {
    // create path and map variables
    val path = (addFmt("/endpoints/{deploymentName}/model/{modelName}/{versionName}/multipredict")
      replaceAll("\\{" + "deploymentName" + "\\}", DeploymentName.toString)
      replaceAll("\\{" + "versionName" + "\\}", VersionName.toString)
      replaceAll("\\{" + "modelName" + "\\}", ModelName.toString))

    // query params
    val queryParams = new mutable.HashMap[String, String]
    val headerParams = new mutable.HashMap[String, String]

    if (Body == null) throw new Exception("Missing required parameter 'Body' when calling InferenceApi->multipredict")
    if (DeploymentName == null) throw new Exception("Missing required parameter 'DeploymentName' when calling InferenceApi->multipredict")

    if (VersionName == null) throw new Exception("Missing required parameter 'VersionName' when calling InferenceApi->multipredict")

    if (ModelName == null) throw new Exception("Missing required parameter 'ModelName' when calling InferenceApi->multipredict")


    val resFuture = client.submit("POST", path, queryParams.toMap, headerParams.toMap, writer.write(Body))
    resFuture flatMap { resp =>
      process(reader.read(resp))
    }
  }

  def multipredictimage(File: File,
    Id: String,
    NeedsPreprocessing: Boolean,
    DeploymentName: String,
    VersionName: String,
    ModelName: String)(implicit reader: ClientResponseReader[MultiPredictResponse]): Future[MultiPredictResponse] = {
    // create path and map variables
    val path = (addFmt("/endpoints/{deploymentName}/model/{modelName}/{versionName}/multipredictimage")
      replaceAll("\\{" + "deploymentName" + "\\}", DeploymentName.toString)
      replaceAll("\\{" + "versionName" + "\\}", VersionName.toString)
      replaceAll("\\{" + "modelName" + "\\}", ModelName.toString))

    // query params
    val queryParams = new mutable.HashMap[String, String]
    val headerParams = new mutable.HashMap[String, String]

    if (File == null) throw new Exception("Missing required parameter 'File' when calling InferenceApi->multipredictimage")
    if (Id == null) throw new Exception("Missing required parameter 'Id' when calling InferenceApi->multipredictimage")

    if (DeploymentName == null) throw new Exception("Missing required parameter 'DeploymentName' when calling InferenceApi->multipredictimage")

    if (VersionName == null) throw new Exception("Missing required parameter 'VersionName' when calling InferenceApi->multipredictimage")

    if (ModelName == null) throw new Exception("Missing required parameter 'ModelName' when calling InferenceApi->multipredictimage")


    val resFuture = client.submit("POST", path, queryParams.toMap, headerParams.toMap, "")
    resFuture flatMap { resp =>
      process(reader.read(resp))
    }
  }

  def predict(Body: Prediction,
    DeploymentName: String,
    VersionName: String,
    ModelName: String)(implicit reader: ClientResponseReader[Prediction], writer: RequestWriter[Prediction]): Future[Prediction] = {
    // create path and map variables
    val path = (addFmt("/endpoints/{deploymentName}/model/{modelName}/{versionName}/predict")
      replaceAll("\\{" + "deploymentName" + "\\}", DeploymentName.toString)
      replaceAll("\\{" + "versionName" + "\\}", VersionName.toString)
      replaceAll("\\{" + "modelName" + "\\}", ModelName.toString))

    // query params
    val queryParams = new mutable.HashMap[String, String]
    val headerParams = new mutable.HashMap[String, String]

    if (Body == null) throw new Exception("Missing required parameter 'Body' when calling InferenceApi->predict")
    if (DeploymentName == null) throw new Exception("Missing required parameter 'DeploymentName' when calling InferenceApi->predict")

    if (VersionName == null) throw new Exception("Missing required parameter 'VersionName' when calling InferenceApi->predict")

    if (ModelName == null) throw new Exception("Missing required parameter 'ModelName' when calling InferenceApi->predict")


    val resFuture = client.submit("POST", path, queryParams.toMap, headerParams.toMap, writer.write(Body))
    resFuture flatMap { resp =>
      process(reader.read(resp))
    }
  }

  def predictimage(DeploymentName: String,
    VersionName: String,
    ModelName: String,
    Image: Option[File] = None
    )(implicit reader: ClientResponseReader[Prediction]): Future[Prediction] = {
    // create path and map variables
    val path = (addFmt("/endpoints/{deploymentName}/model/{modelName}/{versionName}/predictimage")
      replaceAll("\\{" + "deploymentName" + "\\}", DeploymentName.toString)
      replaceAll("\\{" + "versionName" + "\\}", VersionName.toString)
      replaceAll("\\{" + "modelName" + "\\}", ModelName.toString))

    // query params
    val queryParams = new mutable.HashMap[String, String]
    val headerParams = new mutable.HashMap[String, String]

    if (DeploymentName == null) throw new Exception("Missing required parameter 'DeploymentName' when calling InferenceApi->predictimage")

    if (VersionName == null) throw new Exception("Missing required parameter 'VersionName' when calling InferenceApi->predictimage")

    if (ModelName == null) throw new Exception("Missing required parameter 'ModelName' when calling InferenceApi->predictimage")


    val resFuture = client.submit("POST", path, queryParams.toMap, headerParams.toMap, "")
    resFuture flatMap { resp =>
      process(reader.read(resp))
    }
  }

  def predictwithpreprocess(Body: List[String],
    DeploymentName: String,
    VersionName: String,
    ModelName: String)(implicit reader: ClientResponseReader[Prediction], writer: RequestWriter[List[String]]): Future[Prediction] = {
    // create path and map variables
    val path = (addFmt("/endpoints/{deploymentName}/model/{modelName}/{versionName}/predictwithpreprocess")
      replaceAll("\\{" + "deploymentName" + "\\}", DeploymentName.toString)
      replaceAll("\\{" + "versionName" + "\\}", VersionName.toString)
      replaceAll("\\{" + "modelName" + "\\}", ModelName.toString))

    // query params
    val queryParams = new mutable.HashMap[String, String]
    val headerParams = new mutable.HashMap[String, String]

    if (DeploymentName == null) throw new Exception("Missing required parameter 'DeploymentName' when calling InferenceApi->predictwithpreprocess")

    if (VersionName == null) throw new Exception("Missing required parameter 'VersionName' when calling InferenceApi->predictwithpreprocess")

    if (ModelName == null) throw new Exception("Missing required parameter 'ModelName' when calling InferenceApi->predictwithpreprocess")


    val resFuture = client.submit("POST", path, queryParams.toMap, headerParams.toMap, writer.write(Body))
    resFuture flatMap { resp =>
      process(reader.read(resp))
    }
  }

  def predictwithpreprocessjson(Body: List[String],
    DeploymentName: String,
    VersionName: String,
    ModelName: String)(implicit reader: ClientResponseReader[JsonArrayResponse], writer: RequestWriter[List[String]]): Future[JsonArrayResponse] = {
    // create path and map variables
    val path = (addFmt("/endpoints/{deploymentName}/model/{modelName}/{versionName}/predictwithpreprocessjson")
      replaceAll("\\{" + "deploymentName" + "\\}", DeploymentName.toString)
      replaceAll("\\{" + "versionName" + "\\}", VersionName.toString)
      replaceAll("\\{" + "modelName" + "\\}", ModelName.toString))

    // query params
    val queryParams = new mutable.HashMap[String, String]
    val headerParams = new mutable.HashMap[String, String]

    if (DeploymentName == null) throw new Exception("Missing required parameter 'DeploymentName' when calling InferenceApi->predictwithpreprocessjson")

    if (VersionName == null) throw new Exception("Missing required parameter 'VersionName' when calling InferenceApi->predictwithpreprocessjson")

    if (ModelName == null) throw new Exception("Missing required parameter 'ModelName' when calling InferenceApi->predictwithpreprocessjson")


    val resFuture = client.submit("POST", path, queryParams.toMap, headerParams.toMap, writer.write(Body))
    resFuture flatMap { resp =>
      process(reader.read(resp))
    }
  }


}
